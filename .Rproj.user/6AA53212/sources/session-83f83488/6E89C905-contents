# Data Collection and Processing
```{r, tidy=FALSE, message= F, warning=F, error=F, echo=T}
library(magrittr)
library(dplyr)
library(kableExtra)
library(readxl)
library(sjmisc)
library(purrr)
library(tidyr)


main_dataset <- read.csv("inputs/UKR2007_MSNA20_HH_dataset_main_rcop.csv", na.strings = "", stringsAsFactors = F)

questions <- read_xlsx("inputs/UKR2007_MSNA20_HH_Questionnaire_24JUL2020.xlsx",sheet="survey")
choices <- read_xlsx("inputs/UKR2007_MSNA20_HH_Questionnaire_24JUL2020.xlsx",sheet="choices")
```

```{r, tidy=FALSE, eval = F}
#** un-comment those if you need them to be installed
# devtools::install_github("https://github.com/impact-initiatives/xlsformfill/")
# devtools::install_github("https://github.com/impact-initiatives/cleaninginspectoR/")
```

*
Notes on **cleaninginspectoR**:
- The function will return a dataframe with indexes not the uuid.
- cleanninginspectoR uses function from the **magrittr** and **dplyr** packages. Don't forget to load them before using it.
*

## Testing the tool

### Creating dummy data

**xlsformfill** has a function **xlsform_fill** that will create dataset based on your KOBO questionnaire. It takes 3 arguments: your questions, your choices and how many rows you want.

*Notes:
- All questions will be filled. 
- Skip logic are not implemented.
- Constraints are not implemented.
- Integers and text will be generated randomly
*
```{r, tidy=FALSE, message= F, warning=F, error=F, echo=T}
dummy_dataset <- xlsformfill::xlsform_fill(questions = questions, choices = choices, n = 300)
```
```{r, tidy=FALSE, eval = F}
dummy_dataset %>% head(10)
```
```{r, echo =F, message= F, warning=F,}
dummy_dataset  %>% 
  head(10) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

## Download data

### Downloading Audit files using audit_URL
Required Libraries: httr

****

"download_audit_files" will download audit files using the endpoint that is included in your data set, it's available in column called "audit_URL". The function will create a folder named by the uuid of each form/interview, and write the audit.csv inside it. The advantage of using the API over downloading manually from server is that you can download the audit files of each form that has recently been submitted to the server separately, rather than downloading cumulatively the whole audit files each time. One the top of that, you can benefit from the advantage of automation.

```{r}
download_audit_files <- function(df, uuid_column = "_uuid", audit_dir, usr, pass){
  if (!"httr" %in% installed.packages()) 
    stop("The package is httr is required!")
  
  if (is.na(audit_dir) || audit_dir == "") 
    stop("The path for storing audit files can't be empty!")
  
  if (is.na(usr) || usr == "") 
    stop("Username can't be empty!")
  
  if (is.na(pass) || pass == "") 
    stop("Password can't be empty!")
  
  # checking if the output directory is already available
  if (!dir.exists(audit_dir)) {
    dir.create(audit_dir)
    if (dir.exists(audit_dir)) {
      cat("Attention: The audit file directory was created in", audit_dir,"\n")
    }
  }
  
  # checking if creating output directory was successful
  if (!dir.exists(audit_dir))
    stop("download_audit_fils was not able to create the output directory!")
  # checking if uuid column exists in data set
  if (!uuid_column %in% names(df))
    stop("The column ", uuid_column, " is not available in data set.")
  # checking if column audit_URL exists in data set
  if (!uuid_column %in% names(df))
    stop("The column ", uuid_column, " is not available in data set.")
  if (!"audit_URL" %in% names(df))
    stop("Error: the column audit_URL is not available in data set.")
  
  # getting the list of uuids that are already downloaded
  available_audits <- dir(audit_dir)
  
  # excluding uuids that their audit files are already downloaded
  df <- df[!df[[uuid_column]] %in% available_audits,]
  
  audits_endpoint_link <- df[["audit_URL"]]
  names(audits_endpoint_link) <- df[[uuid_column]]
  audits_endpoint_link <- na.omit(audits_endpoint_link)
  
  if (length(audits_endpoint_link) > 0) {
    # iterating over each audit endpoint from data
    for (i in 1:length(audits_endpoint_link)) {
      uuid = names(audits_endpoint_link[i])
      endpoint_link_i <- audits_endpoint_link[i]
      cat("Downloading audit file for", uuid, "\n")
      
      # requesting data
      audit_file <- content(GET(endpoint_link_i,
                                authenticate(usr, pass),
                                timeout(1000),
                                progress()), "text", encoding = "UTF-8")
      
      if (!is.na(audit_file)) {
        if (length(audit_file) > 2) {
          dir.create(paste0(audit_dir, "/", uuid), showWarnings = F)
          write.csv(audit_file, paste0(audit_dir, "/", uuid, "/audit.csv"), row.names = F)
        }else if(!audit_file == "Attachment not found"){
          if (grepl("[eventnodestartend]", audit_file)) {
            dir.create(paste0(audit_dir, "/", uuid), showWarnings = F)
            write.table(audit_file, paste0(audit_dir, "/", uuid, "/audit.csv"), row.names = F, col.names = FALSE, quote = F)
          } else{
            cat("Error: Downloading audit was unsucessful!\n")
          }
        }
      } else{
        cat("Error: Downloading audit was unsucessful!\n")
      }
    }
  } else{
    cat("Attention: All audit files for given data set is downloaded!")
  }
}

```

### Download audit files using form name
Required Libraries: httr, jsonlite, dplyr, stringr

****

"download_audit_files_2" is a different version of "download_audit_files" module that will help you to download audit files using the form id directly from KOBO API. The rest of it's specifications best matches the download_audit_files.

```{r}
download_audit_files_2 <- function(form_id, output_dir, user, pass) {
  if (!"stringr" %in% installed.packages()) 
    stop("Could not find the package stringr!")
  
  if (!"dplyr" %in% installed.packages()) 
    stop("Could not find the package dplyr!")
  
  if (!"httr" %in% installed.packages()) 
    stop("Could not find the package httr!")
  
  if (!"jsonlite" %in% installed.packages()) 
    stop("Could not find the package jsonlite!")
  
  if (is.na(output_dir) || output_dir == "") 
    stop("The path for storing audit files can't be empty!")
  
  if (is.na(user) || user == "") 
    stop("Username can't be empty!")
  
  if (is.na(pass) || pass == "") 
    stop("Password can't be empty!")
  
  require(httr)
  require(dplyr)
  require(jsonlite)
  require(stringr)
  
  # checking if the output directory is already available
  if (!dir.exists(output_dir)) {
    dir.create(output_dir)
    if (dir.exists(output_dir)) {
      cat("Attention: The audit file directory was created in", output_dir,"\n")
    }
  }
  
  # checking if creating output directory was successful
  if (!dir.exists(output_dir))
    stop("download_audit_fils was not able to create the output directory!")
  
  # listing audit files that are already available
  available_audits <- dir(output_dir)
  
  # getting form url
  form_url <- GET("https://kc.humanitarianresponse.info/api/v1/data", authenticate(user, pass), timeout(1000)) %>% 
    content(., "text", encoding = "UTF-8") %>%
    fromJSON(., flatten = T) %>% 
    filter(id_string == form_id) %>% 
    select(url) %>% 
    unlist()
  
  if (length(form_url) > 0) {
    
    # getting attachment links of uuids that their audit is already downloaded
    form_data <- GET(form_url, authenticate(user, pass), timeout(1000)) %>% 
      content(., "text", encoding = "UTF-8") %>%
      fromJSON(., flatten = T) %>% 
      as.data.frame() %>% 
      filter(!`_uuid` %in% available_audits)
    
    attachment_url <- form_data[["_attachments"]]
    audit_file_links <- unlist(attachment_url)[names(unlist(attachment_url)) == "download_large_url"] %>% unname()
    
    # uuids <- form_data[["_uuid"]][lapply(attachment_url, length) %>% unlist() != 0]
    # uuids <- str_extract(audit_file_links, "(?<=%2F)[a-z-0-9]*(?=%2Faudit.csv$)")
    # names(audit_file_links) <- uuids
    
    download_audit <- function(audit_link, user, pass, output_dir, uuid) {
      uuid <- str_extract(audit_link, "(?<=%2F)[a-z-0-9]*(?=%2Faudit.csv$)")
      audit_file <- GET(audit_link, authenticate(user, pass), timeout(1000), progress()) %>% 
        content(., "text", encoding = "UTF-8")
      
      cat("Downloading audit file for", uuid, "\n")
      dir.create(paste0(output_dir, "/", uuid))
      output_file_name <- paste0(output_dir, "/", uuid,"/audit.csv")
      # write.csv(audit_file, output_file_name)
      
      if (!is.na(audit_file)) {
        if (length(audit_file) > 2) {
          write.csv(audit_file, output_file_name, row.names = F)
        }else if(!audit_file == "Attachment not found"){
          if (grepl("[eventnodestartend]", audit_file)) {
            write.table(audit_file, output_file_name, row.names = F, col.names = FALSE, quote = F)
          }
        }
      }
    }
    
    downloaded_uuids <- sapply(audit_file_links, download_audit, user = user, pass = pass, output_dir = output_dir)
  } else{
    cat("Attention: All audit files for given form is downloaded!")
  }
}
```

## Data collection follow-up

### checking surveys against the sampling frame
It is important to ensure that number of interviews that have been collected in each sampling unit corresponds with the initial sampling frame. The best choice will be to check it on daily basis during the data collection and follow-up with the Field Team in case there were not enough interviews (or some extra surveys) in certain locations. But it is also a good practice to make a final check to understand the overall difference between the sampling frame and collected data.

First let's open initial sampling frame generated by the [Probability sampling tool](https://impact-initiatives.shinyapps.io/r_sampling_tool_v2/)
```{r}
sampling_frame <- read.csv("inputs/sampling_frame20200701-132150.csv")
```

Now we should create summary table that will show number of interviews actually collected for each primary sampling unit (in our case it was settlement).
*Keep in mind that you should use correct settlement codes that were verified during the data collection. In case you are not sure that settlement codes in the dataset are correct you should cross-check them with GPS-coordinates using [Spatial verification checks]*
```{r}
samples_collected <- main_dataset %>%
                    group_by(r3_current_settlement)%>%
                    count()
```

The next step will be left join of samples collected dataset to the sampling frame and finding the difference in collected surveys for each settlement.
```{r}
sampling_frame <- sampling_frame %>%
  select(Survey, strata_id, adm4Pcode, adm4NameLat) %>%
  left_join(samples_collected, by = c("adm4Pcode" = "r3_current_settlement"))%>%
  tidyr::replace_na(list(n = 0))%>%
  mutate(sample_difference = n - Survey)%>%
  arrange(sample_difference)

sampling_frame %>% 
      head(10) %>%
      kable() %>% 
      kable_styling(font_size=12)
```

As we can see from the table above, there was only one settlement where the target number of the interviews was not collected. And considering that it's only one survey it will not affect our results and we can proceed with other checks.
After checking how a number of collected surveys corresponds with the sampling frame for each primary sampling unit it will be also good to make the same check on strata level.
To do that we can use [summarise](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/summarise) function from [dplyr library](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8)
```{r}
strata_level_check <- sampling_frame %>%
                      group_by(strata_id)%>%
                      summarise(Survey_Planned = sum(Survey),
                                Survey_Collected = sum(n))%>%
                      mutate(Difference = Survey_Collected - Survey_Planned)%>%
                      arrange(desc(Difference))

strata_level_check %>% 
      kable() %>% 
      kable_styling(font_size=12)

```
After this check, we can see that for two strata we collected the exact number of interviews that were initially planned. And for two strata there is slight overachievement in 4 and 3 surveys.

## Data falsification

### Check for time (in the dataset)

Required Libraries: "dplyr" and "lubridate"

****
  
"time_check" will return the elapsed time for each interview based on it's start and end columns also classifies if it's "too short", "too long", or "okay".
As an example, we will use a dummy data set to apply the function to it. The function also needs a time_min (the minimum time in minutes that an interview should take to be completed) and a time_max (the maximum time in minutes that an interview should take to be completed) parameters.
```{r, eval=T}
library(dplyr)
library(lubridate)

# Creating a dummy data set
start <- c("2020-12-01T09:40:05.750+04:30","2020-12-01T09:40:18.709+04:30","2020-12-01T09:45:40.879+04:30","2020-12-01T09:46:28.328+05:00")
end <- c("2020-12-01T09:44:44.438+04:30","2020-12-01T10:01:27.890+04:30","2020-12-01T10:17:44.021+04:30","2020-12-01T10:18:32.717+05:00")

my_df <- data.frame(start, end)

# Initializing variables
time_min <- 8
time_max <- 30

# declaring the function
time_check <- function(df, time_min, time_max){
  df <- df %>% mutate(interview_duration = difftime(as.POSIXct(ymd_hms(end)), as.POSIXct(ymd_hms(start)), units = "mins"),
                      CHECK_interview_duration = case_when(
                        interview_duration < time_min ~ "Too short",
                        interview_duration > time_max ~ "Too long",
                        TRUE ~ "Okay"
                      )
  )
  return(df)
}

# Applying the function to data frame
processed_df <- time_check(my_df, time_min, time_max)
```
```{r, tidy=FALSE, eval = F}
processed_df
```
```{r, echo =F, message= F, warning=F,}
processed_df %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
****

### Check for time (audit files)	

  Required Libraries: "dplyr", "lubridate", and "expss"

****
  
  "time_check_audit" will calculate the interview duration using the audit files. And, if the audit file for that particular uuid is not found in the audit directory (where you paste the audit files), it will calculate it using start and end time columns in the data set.
Audit files should be stored inside a folder (to avoid redefining its name while calling the function, call it "audit_files") in the project folder.
```{r, eval = T, message = F, warning = F, results = 'hide'}
library(dplyr)
library(lubridate)
library(expss)

# reading files.
main_dataset <- read.csv("inputs/UKR2007_MSNA20_HH_dataset_main_rcop.csv", na.strings = "")

# Initializing variables
time_min <- 8
time_max <- 30


# declaring the function
time_check_audit <- function(df_raw, x_uuid="_uuid", time_min, time_max, audit_dir_path = "./audit_files/", today = "today"){
  if (!any(duplicated(df_raw[[x_uuid]]))) {
    # Audit Checks
    # audit_dir_path < -audit_dir_path
    uuids <- dir(audit_dir_path)
    dfl <- list()
    all_uuids <- length(uuids)

    for(i in 1: length(uuids)){
      df <- read.csv(paste0(audit_dir_path, uuids[i], "/audit.csv"))
      df <- df %>% filter(node != "")
      duration_ms <- sum(df$end - df$start)
      duration_secs <-duration_ms/1000
      duration_minutes <- round(duration_secs/60,1)
      dfl[[i]] <- data.frame(uuid = uuids[i], duration_ms=duration_ms, 
                             durations_secs=duration_secs, duration_minutes = duration_minutes)
      cat("\014","Running audit: ", round((i/all_uuids) * 100,0),"%\n", sep = "")
    }
    duration_df <- do.call("rbind", dfl)
    duration_df <- dplyr::rename(duration_df, `_uuid` = uuid)
    
    #time check based on start end time
    df_str_audit_all <- df_raw %>% 
      mutate(start_end = difftime(as.POSIXct(ymd_hms(end)), as.POSIXct(ymd_hms(start)), units = "mins")) 
    
    #creating a binding column with same name.
    df_str_audit_all$`_uuid` <- df_str_audit_all[[x_uuid]]

    # Join Audit checks and main data set
    df_str_audit_all <- df_str_audit_all %>%
      left_join(select(duration_df, `_uuid`, duration_minutes), by =  "_uuid")
    
    # Checking time duration with audit file, if not available, from dataset start/end.
    df_str_audit_all <- df_str_audit_all %>%
      mutate(interview_duration = if_na(duration_minutes, start_end),
             CHECK_interview_duration = case_when(
               interview_duration < time_min ~ "Too short",
               interview_duration > time_max ~ "Too long",
               TRUE ~ "Okay")
      ) %>% select( -c(duration_minutes,start_end))
    
    return(df_str_audit_all)
  }else{
    stop("Error: df_raw has duplicate in uuid column, resolve the duplication to proceed!")
  }
}

# Applying the function to data frame
processed_df <- time_check_audit(main_dataset, 
                                 x_uuid = "X_uuid",
                                 time_min, time_max, 
                                 audit_dir_path = "inputs/reach_global/attachments/dc4b0f40bf934293aedd3f31ff43f6d1/")
```
```{r, tidy=FALSE, eval = F}
processed_df %>%
  select(`_uuid`, interview_duration, CHECK_interview_duration) %>% 
  head(10)
```
```{r, echo =F, message= F, warning=F,}
processed_df %>% 
  select(`_uuid`, interview_duration, CHECK_interview_duration) %>% 
  head(10) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```


****

### Check for time - calculating the elapsed time between each interview

Required Libraries: lubridate

****
  
"time_btwn_ints" will calculate the elapsed time between the ending time of the first interview of an enumerator and the start time of its second interview, and the process applies for all interviews of each enumerator.
It needs a location identifier as a parameter to check if the elapsed time is matching with the threshold (given as a parameter) in the same location or not. Also it will be checked if the elapsed time matches the given threshold for interviews in different locations by the same enumerator.
```{r, eval = T}
library(lubridate)

# Creating a dummy data set
start <-  c("2020-12-01T09:40:05.750+04:30",
            "2020-12-01T09:45:18.709+04:30",
            "2020-12-01T09:45:40.879+04:30",
            "2020-12-01T10:02:40.879+04:30",
            "2020-12-01T10:25:28.32+04:30")
end <-  c("2020-12-01T09:44:44.438+04:30",
          "2020-12-01T10:01:27.890+04:30",
          "2020-12-01T10:17:44.021+04:30",
          "2020-12-01T10:05:40.879+04:30",
          "2020-12-01T10:38:32.717+04:30")
device_id <- c("000215",
               "000215",
               "000216",
               "000215",
               "000216")
village <- c("Village A",
             "Village A",
             "Village B",
             "Village A",
             "Village C")

my_df <- data.frame(start, end, device_id, village)

# declaring the function
time_btwn_ints <- function(df, device_id, start_col = "start", end_col = "end", 
                           village_col, same_village_threshold = 3, diff_village_threshold = 5){
  checked_df <- df
  
  # sort by device_id and start_col
  checked_df <- checked_df[order(checked_df[[start_col]]), ]
  checked_df <- checked_df[order(checked_df[[device_id]]), ]
  
  # For each row starting from the second row:
  # 1) calculate the time between the end of the (r-1) survey and the start of the (r) survey
  # 2) insert the eight check-message based on the calculated time and the village
  issue.same.village <- paste0("The elapsed time between two interviews in the same village is less than ",same_village_threshold, " minutes")
  issue.diff.village <- paste0("The elapsed time between two interviews in different villages is less than ", diff_village_threshold, " minutes")
  checked_df$check <- "OK"
  checked_df$gap_between_ints <- NA
  for (r in 2:nrow(checked_df)){
    if (as.character(checked_df[r, device_id])==as.character(checked_df[r-1, device_id])){
      checked_df$gap_between_ints[r] <- difftime(as.POSIXct(ymd_hms(checked_df[r, start_col])),
                                                 as.POSIXct(ymd_hms(checked_df[r-1, end_col])),
                                                 units = "mins")
      
      if (as.character(checked_df[r, village_col])==as.character(checked_df[r-1, village_col])){
        if (checked_df$gap_between_ints[r] < same_village_threshold) checked_df[r, "check"] <- issue.same.village
      } else{
        if (checked_df$gap_between_ints[r] < diff_village_threshold) checked_df[r, "check"] <- issue.diff.village
      }
    }
  }
  
  return(checked_df)
}

# Applying the function to data frame
processed_df <- time_btwn_ints(df = my_df, device_id = "device_id",village_col = "village", same_village_threshold = 2,diff_village_threshold = 10)

```
```{r, tidy=FALSE, eval = F}
processed_df
```
```{r, echo =F, message= F, warning=F,}
processed_df %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

****

### Silouhette analysis	
Custom script (using ???)

### Check for duplicates	
#### cleaninginspectoR - find_duplicates

**find_duplicates** will take the dataset and a column name to look for duplicates as arguments.

```{r, tidy=FALSE, message= F, warning=F, error=F}
cleaninginspectoR::find_duplicates(main_dataset, duplicate.column.name = "X_uuid")

dummy_dataset[301, ] <- dummy_dataset[300, ]
cleaninginspectoR::find_duplicates(dummy_dataset, duplicate.column.name = "uuid")
```
```{r, tidy=FALSE, message= F, warning=F, error=F}
cleaninginspectoR::find_duplicates(dummy_dataset, duplicate.column.name = "start")
```

#### base - duplicated

If you are looking in duplicates value in several columns (first and second name, names and ID number,etc.), you can use the **duplicated**. 

```{r, tidy=T}
dummy_test <- data.frame(col_a = c("a", "a", "c"), 
                         col_b = c("b", "b", "f"))
dummy_test
```
Rows 1 and 2 are duplications. 
```{r, tidy=FALSE, message= F, warning=F, error=F}
duplicated(dummy_test)
```

**find_duplicates()** and **duplicated()** functions will return position or value **only of one** duplicated record. But after identification of the duplicate, it will be good to check how many of such duplicated records in the dataset and check if they have any other duplicated columns. Based on your investigation, you will need to delete one or several records. e.g. Enumerator submitted the first survey by mistake and after some time submitted a corrected survey with the same id (in case we allow for the enumerator to select the id of the enterprise or sample). In such a way, find_duplicates() will identify the second survey but we will need to delete the first one.


#### Find most similar surveys

The function **find_similar_surveys()** compares each survey with each other survey in the dataset and finds the most similar one, i.e., the one with the lowest number of different answers. The function uses the gower matrix to make the comparison more efficient. The function returns a dataframe with the same number of rows (all surveys) and a few additional columns indicating the ID of the most similar survey and how many columns are different.

Depending on the size of the questionnaire and on the data collection methodology, we can set a maximum threshold on the number of differences and follow up on all the surveys that have a matching survey with a lower number of differences than the threshold. For example, in the MSNA in Syria, we used a maximum threshold of 7 differences.

In the version below, the function uses only the data from the main sheet of the survey. If the tools includes loop(s), it makes sense to add a few relevant columns from the loops to the main dataframe so that they are also used in the search of the most similar survey. For example, for an HH survey with a loop for the HH composition, one can add to the main dataframe one column with the concatenation of the genders of the HH components and one column with the concatenation of the ages of the HH components (from the loop).

```{r, eval=F}
find_similar_surveys <- function(raw.data, tool.survey, uuid="_uuid"){
  # 1) store UUIDs
  uuids <- raw.data[[uuid]]
  
  # 2) convert all columns to character and tolower
  raw.data <- mutate_all(raw.data, as.character)
  raw.data <- mutate_all(raw.data, tolower)
  
  # 3) remove columns that are naturally different in each survey:
  # - columns of type = "start", "end", "text" (for the other responses), etc.
  # - columns starting with "_"
  # - option columns for the select multiple -> keeping only the concatenation column
  types_to_remove <- c("start", "end", "today", "deviceid", "date", "geopoint", "audit", 
                       "note", "calculate", "text")
  cols_to_keep <- data.frame(column=colnames(raw.data)) %>% 
    left_join(select(tool.survey, name, type), by=c("column"="name")) %>% 
    filter(!(type %in% types_to_remove) & !str_starts(column, "_") & !str_detect(column, "/"))
  raw.data <- raw.data[, all_of(cols$column)]
  
  # 4) remove columns with all NA; convert remaining NA to "NA"; convert all columns to factor
  raw.data <- raw.data[, colSums(is.na(raw.data))<nrow(raw.data)]
  raw.data[is.na(raw.data)] <- "NA"
  raw.data <- raw.data %>% mutate_if(is.character, factor)
  error.message <- "NAs detected, remove them before proceeding (it can happen when converting to factor)"
  if (sum(is.na(raw.data))>0) stop(error.message)
  
  # 5) calculate gower distance
  gower_dist <- daisy(raw.data, metric="gower", warnBin=F, warnAsym=F, warnConst=F)
  gower_mat <- as.matrix(gower_dist)
  
  # 6) convert distance to number of differences and determine closest matching survey
  r <- unlist(lapply(1:nrow(raw.data), function(i) sort(gower_mat[i,]*ncol(raw.data))[2]))
  
  # 7) add relevant columns
  raw.data[["num_cols_not_NA"]] <- rowSums(raw.data!="NA")
  raw.data[[uuid]] <- uuids
  raw.data[["_id_most_similar_survey"]] <- uuids[as.numeric(names(r))]
  raw.data[["number_different_columns"]] <- as.numeric(r)
  raw.data <- raw.data %>% arrange(number_different_columns, uuid)
  
  return(raw.data)
}
```


##	Data checks	
### Check for outliers	
There are 2 commons ways to detect outliers :

- Using the range of 3 standards deviations from the mean. 
- Using the range of 1.5 inter quartile from the 1st and 3rd quartile. 

Outliers can exist but it is important to check them.

#### cleaninginspectoR - find_outliers
The function find_outliers will use the rule of the 3 standards deviations from the mean for normal values and log values. 

```{r, tidy=FALSE, message= F, warning=F, error=F, echo=T}
cleaning_log <- cleaninginspectoR::find_outliers(main_dataset)
```
```{r, tidy=FALSE, eval = F}
cleaning_log %>% head(20)
```
```{r, echo =F, message= F, warning=F,}
cleaning_log  %>% 
  head(20) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

###	Check others
#### cleaninginspectoR - find_other_responses

**find_other_responses** will look for all columns with "other", "Other", "autre", "Autre",  and return their values.

*Notes:
- If your *other* questions do not have those 4 strings in their names, the function will not pick it.
*
```{r, tidy=FALSE, message= F, warning=F, error=F, echo=T}
cleaning_log <- cleaninginspectoR::find_other_responses(main_dataset)
```
```{r, tidy=FALSE, eval = F}
cleaning_log %>% head(10)
```
```{r, echo =F, message= F, warning=F,}
cleaning_log  %>% 
  head(10) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

#### base + dplyr

This example will take all the text type from the questionnaire, filter for the ones that are in the dataset and return all the values. 
```{r, tidy=FALSE, echo=T}
oth <- questions$name[questions$type == "text"]
oth <- oth[oth %in% names(main_dataset)]
oth_log <- oth %>% 
  lapply(function(x) {
    main_dataset %>% 
      select("X_uuid", !!sym(x)) %>% 
      filter(!is.na(!!sym(x))) %>%
      as.data.frame() %>% 
      mutate(col_names = x) %>%
      rename(other_text = !!sym(x)) %>%
      arrange(other_text)}) %>% 
  do.call(rbind,.)
```
```{r, tidy=FALSE, eval = F}
oth_log %>% head(20)
```
```{r, echo =F, message= F, warning=F,}
oth_log  %>% 
  head(20) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

*Please note that it takes the values as they are. You may want to trim and remove caps or any other regex work if you want better summary *

This other example looks at the frequency of a given *other* option, it could be used to see if some should be recoded as options directly.

```{r, tidy=FALSE, message= F, warning=F, error=F, echo=T}
oth_prop <- oth %>% 
  lapply(function(x) {
    main_dataset %>% 
      group_by(!!sym(x)) %>% 
      tally(sort = T) %>% 
      rename(other_text = !!sym(x)) %>% 
      filter(!is.na(other_text)) %>%
      mutate(col_names = x, 
             prop = n/nrow(main_dataset))
    }) %>% 
  do.call(rbind,.)
```
```{r, tidy=FALSE, eval = F}
oth_prop %>% head(20)
```
```{r, echo =F, message= F, warning=F,}
oth_prop  %>% 
  head(20) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```


### Issue log Function

this function will help the generation of standard cleaning log file that holds issues that need clarification. first we need to subset or filter data based on the check list, then that is the log_sheet function will help us to transform that subsetted data into standard cleaning log format. 

```{r}
log_sheet <- function(data, question.name, issue, action){
  cleaning_logbook <- data.frame("uuid" = as.character(),
                                 "question.name" = as.character(),
                                 "issue" = as.character(),
                                 "feedback" = as.character(),
                                 "action" = as.character(),
                                 "old.value" = as.character(),
                                 "new.value" = as.character(),
                                 stringsAsFactors = F)
  if(nrow(data) > 0){
    for(a in 1:nrow(data)) {
      cleaning_logbook <- cleaning_logbook %>% 
        add_row(
          tibble_row(
            uuid = as.character(data[a, "X_uuid"]),
            question.name = as.character(question.name),
            issue = as.character(issue),
            feedback = as.character(""),
            action = as.character(action),
            old.value = as.character(data[a, question.name]),
            new.value = as.character("")
            
          )
        )
    }
  }
  return(cleaning_logbook)
}

```

Now, lets use the function to log an issue where consent to calls follow-up is no and log them into standard cleaning log format

```{r}
issue_file <- main_dataset %>% 
  filter(a3_consent_to_follow_up_calls == "no") %>% 
  log_sheet(question.name = "a3_consent_to_follow_up_calls", 
            issue = "flagging consent followup calls with no response as an example",
            action = "flag")
```



###	Check for logical check	
hum hum hum ?? 
Custom script (using dplyr?)
		
- Any value that is arbitrary set (from an informed source e.g. an informal setttlement cannot be lower than 15 households). This type of outliers could also be considered as logical checks.

## cleaninginspectoR - inspect_all

cleanninginspectoR has a function inspect *inspect_all* that will look for outliers, others responses that may need recoding, duplicated uuid and possible sensitive columns. It takes as arguments the dataset and the uuid column name. 
```{r, tidy=FALSE, message= F, warning=F, error=F}
cleaning_log <- cleaninginspectoR::inspect_all(main_dataset, "X_uuid")
```
```{r, tidy=FALSE, eval = F}
cleaning_log %>% head(20)
```
```{r, echo =F, message= F, warning=F,}
cleaning_log  %>% 
  head(20) %>%
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

## Data cleaning		
### Re-generate text column for select multiple questions 
**Description**: During data cleaning (when recoding the other options for example), there is a high chance that the dummies and text columns become inconsistent. That can cause issues when analyzing the data if both the dummies and text columns are used. To avoid that, the following function can be used to regenerate and update the text columns based on the dummies columns.

**Usage**:
1- Create a list of the questions to regenerate: 

```{r}
select_multiple_questions = c("b10_hohh_vulnerability","f17_store_drinking_water","b11_hohh_chronic_illness")
```

2- Define the function 
```{r}
generate_from_binaries <- function(data,select_multiple_questions) {
  
  do.call("cbind",map(select_multiple_questions,function(question_name,data) {
    
    df <- data %>% select(grep(paste0("^",question_name,"\\."), names(data))) ## Subseting the dataset to include only dummies related to one question
    df <- mutate_all(df,as.numeric) ## Making sure the dummies columns are numerical
    colnames(df) <- gsub(paste0(question_name,"\\."),"",colnames(df)) ## Keeping only the options names in the colnames
    df <- map2_df(df, names(df), ~  replace(.x, .x==1, .y) %>% replace(. == 0, NA)) ## Replacing a cell with a value of '1' with its respective column name and '0' with NA
    df %>%
      unite(!!sym(question_name),names(df),remove=TRUE,na.rm=TRUE,sep=" ") %>% ## concatenate the columns using " " as a seperator 
      as.data.frame() %>% ## Convert to dataframe
      mutate_all(list(~na_if(.,""))) ## replace empty string with NA
  },data)) 
  
}
```

3- Call the function the generate the columns and replace directely in the data set using replace_columns

```{r}
main_dataset <- replace_columns(main_dataset, generate_from_binaries(main_dataset,select_multiple_questions)) 
```


### Clean data base on cleaning log	
Required Libraries: Base R Packages

****
  
  The function "incorporate_logs" applies cleaning log on raw data. It gets cleaning log in a format that must contain 5 mandatory columns including uuid, question.name, old.value, new.value, and changed. as result it would return the cleaned data frame, master cleaning log (logs that are both applied and not applied on data), the version of cleaning log that was applied on raw data, a report of duplicate logs, and logs that their question name or uuid is not available in raw data frame.

****

```{r, eval = T}
# Creating a dummy data set
city_name <- c("kabul", "new dlehi", "peshawar","new york")
population <- c(4430000, 21750000, 1970000, 8419000)
uuid <- c("eae001", "eae002", "eae003","eae004")

my_df <- data.frame(city_name, population, uuid)

# Creating a dummy cleaning log
old.value <- c("kabul", 4430000, "europe","kabul")
question.name <- c("city_name", "population", "continent","city_name")
new.value <- c("moscow",11920000,"asia","moscow")
uuid <- c("eae001","eae001","eae001","eae001")
changed <- c("yes","yes","yes","yes")

cleaning_log <- data.frame(uuid,question.name, old.value, new.value, changed)

### declaring the function
incorporate_logs = function(raw_df, cleaning_log, df_group_seprator = "/", uuid_col = "_uuid"){
  error <- "Error!
Execution was haulted due to one of the following issues:
  - Cleaning log is empty
  - There is no changes in data (in cleaning log changed property for all logs is set to 'NO')
  - One/morethan one of the (uuid, question.name, old.value, new.value, and changed) columns are missing or column names are misspelled
"
  if (sum(grepl("uuid|question.name|old.value|new.value|changed", names(cleaning_log)))==5) {
    `%nin%` = Negate(`%in%`)
    # changing the group seprator (operator) from "/" to "."
    names(raw_df) <- gsub(df_group_seprator,".",names(raw_df))
    cleaning_log$question.name <- gsub(df_group_seprator,".", cleaning_log$question.name)
    
    # subsetting logs that their question is not (available) in dataset
    logs_not_in_rawdf <- cleaning_log[cleaning_log$question.name %nin% names(raw_df) | cleaning_log$uuid %nin% raw_df[[uuid_col]], ]
    logs_not_in_rawdf <- logs_not_in_rawdf[logs_not_in_rawdf$changed %in% c("yes","Yes"),]
    
    # subsetting logs that their question exist in raw data frame and its new value is changed
    cleaning_log.changed <- cleaning_log[cleaning_log$question.name %in% names(raw_df) & cleaning_log$uuid %in% raw_df[[uuid_col]], ]
    cleaning_log.changed <- cleaning_log.changed[cleaning_log.changed$changed %in% c("yes","Yes"),]
    
    # capturing duplicate logs
    cleaning_log$unique_key <- paste(cleaning_log$uuid, cleaning_log$question.name, sep = "_")
    duplicate_logs <- cleaning_log[(duplicated(cleaning_log$unique_key) | duplicated(cleaning_log$unique_key, fromLast = T)),]
    
    # cleaning master cleaning log
    cleaning_log <- cleaning_log[cleaning_log$uuid %nin% logs_not_in_rawdf$uuid | cleaning_log$question.name %nin% logs_not_in_rawdf$question.name,]
    cleaning_log <- cleaning_log[!is.na(cleaning_log$question.name), ]
    cleaning_log <- cleaning_log[!is.na(cleaning_log$uuid), ]
    
    raw_df_valid <- raw_df
    if (nrow(cleaning_log.changed)>0) {
      # Apply cleaning log on raw data
      for (rowi in 1:nrow(cleaning_log.changed)){
        uuid_i <- cleaning_log.changed$uuid[rowi]
        var_i <- cleaning_log.changed$question.name[rowi]
        old_i <- cleaning_log.changed$old.value[rowi]
        new_i <- cleaning_log.changed$new.value[rowi]
        if(class(raw_df_valid[[var_i]]) == "character"){
          new_i <- as.character(new_i)
        }else if(class(raw_df_valid[[var_i]]) == "numeric"){
          new_i <- as.numeric(new_i)
        }else if(class(raw_df_valid[[var_i]]) == "logical"){
          new_i <- as.integer(new_i)
        }else if(class(raw_df_valid[[var_i]]) == "integer"){
          new_i <- as.integer(new_i)
        }
        # Find the variable according to the row of the cleaning log
        raw_df_valid[raw_df_valid[[uuid_col]] == uuid_i, var_i] <- new_i
        print(paste(rowi,"uuid:", uuid_i, "Old value:", old_i, "changed to", new_i, "for", var_i))
      }
      return(list(cleaned_df = raw_df_valid, cleaning_log.applied = cleaning_log.changed, logs_not_in_rawDF = logs_not_in_rawdf, duplicate_logs = duplicate_logs, master_cleaning_log = cleaning_log))
    }else{
      cat(error)
      return(list(cleaned_df = raw_df_valid, cleaning_log.applied = cleaning_log.changed,logs_not_in_rawdf = logs_not_in_rawdf))
    }
  }else{
    cat(error)
  }
}

### Applying the function to data frame
incorprated_logs <- incorporate_logs(my_df, cleaning_log, uuid_col = "uuid")

cleaned_data <- incorprated_logs$cleaned_df
master_cleaning_log <- incorprated_logs$master_cleaning_log
logs_not_in_rawDf <- incorprated_logs$logs_not_in_rawDF
cleaning_log.applied <- incorprated_logs$cleaning_log.applied
duplicate_log <- incorprated_logs$duplicate_logs

```
```{r, tidy=FALSE, eval = F}
cleaned_data
```
```{r, echo =F, message= F, warning=F,}
cleaned_data %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
```{r, tidy=FALSE, eval = F}
master_cleaning_log
```
```{r, echo =F, message= F, warning=F,}
master_cleaning_log %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
```{r, tidy=FALSE, eval = F}
logs_not_in_rawDf
```
```{r, echo =F, message= F, warning=F,}
logs_not_in_rawDf %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
```{r, tidy=FALSE, eval = F}
cleaning_log.applied
```
```{r, echo =F, message= F, warning=F,}
cleaning_log.applied %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
```{r, tidy=FALSE, eval = F}
duplicate_log
```
```{r, echo =F, message= F, warning=F,}
duplicate_log %>% 
  kable() %>% 
  kable_styling(font_size=8) %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
****

### Check cleaning log, raw dataset and clean dataset	
dplyr
waldo 
arsenal

## Data cleaning - miscellaneous		
###	Check for data sanity	check if the data follow ODK format (select_one, select_multiple, xxx, choices)
To be created

###	Turns label to xml	
Custom script (using ???)

###	Statistical Disclosure Control Methods	
Statistical Disclosure Control techniques can be defined as the set of methods to reduce the risk of disclosing information on individuals or organizations.


Statistical Disclosure Control Process

1. Measuring the disclosure risk
Disclosure risk occurs if an unacceptably narrow estimation of a respondentâ€™s confidential information is possible or if exact disclosure is possible with a high level of confidence.

so we'll need to clasify the variables into three categories;

* Non-identifying variables (e.g. respondent feelings)
* Direct identifiers (e.g. respondent names, phone numbers)
* Quasi-identifiers (e.g. age, gender, gps coordinates)

we'll use the main_dataset for demonstrating the process

```{r, echo = F}
# load the SdcMicro package 
library(sdcMicro)

# 
selectedKeyVars <- c("a2_hh_representative_name", # direct identifiers
                     "a3_1_phone", # direct identifiers
                     "b4_gender", # quasi identifiers 
                     "b5_age", # quasi identifiers
                     "b8_hohh_sex", # quasi identifiers
                     "b9_hohh_marital_status", # quasi identifiers
                     "X_r6_gpslocation_latitude", # quasi identifiers
                     "X_r6_gpslocation_longitude", # quasi identifiers
                     "X_r6_gpslocation_precision") # quasi identifiers
```


2. Applying anonymization methods
Sometimes we may have some direct identifier variables that feed our analysis plans and in that cases we will need to deduct data by categorizing continuous variables. 

3. Measuring utility and information loss
```{r}
# weight variable
weightVars <- c('stratum.weight')

# checking risk
objSDC <- createSdcObj(dat = main_dataset, 
                       keyVars = selectedKeyVars, weightVar = weightVars)


print(objSDC, "risk")

#Generate an internal report
#report(objSDC, filename = "disclosure_risk_report",internal = T, verbose = TRUE) 

```


